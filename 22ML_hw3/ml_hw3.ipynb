{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기계학습 - 2022년 2학기\n",
    "### 과제2. 다중계층 신경망을 이용한 얼굴 표정 분류기 작성\n",
    "\n",
    "과제 문의: 전북대학교 컴퓨터공학부 시각 및 학습 연구실 (공과대학 7호관 7619)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- Python >= 3.6\n",
    "- numpy\n",
    "- matplotlib\n",
    "- jupyterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 과제에서는 사람 얼굴의 표정 데이터셋(Toronto Faces Dataset, [TFD](http://aclab.ca/users/josh/TFD.html))을 분류하는 다중계층 신경망(Multi-Layer Neural Net)을 구현하고 테스트 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import imp\n",
    "try:\n",
    "    imp.find_module('jupyterplot')\n",
    "except ImportError:\n",
    "    %pip install jupyterplot\n",
    "    pass\n",
    "\n",
    "from jupyterplot import ProgressPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toronto Faces Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TFD는 1-Anger, 2-Disgust, 3-Fear, 4-Happy, 5-Sad, 6-Suprise, 7-Neutral의 총 7개의 클래스를 가진 데이터셋입니다.\n",
    "\n",
    "데이터셋은 학습, 검증, 테스트(training, validation, test)를 위해서 각각 3374, 419, 385장의 48 $\\times$ 48 크기 grayscale 이미지를 제공합니다.\n",
    "\n",
    "데이터셋의 예시를 확인하기 위해 아래 셀들을 실행해보시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Please DO NOT DELETE this cell. ###\n",
    "\n",
    "def LoadData(fname):\n",
    "    \"\"\"Loads data from an NPZ file.\n",
    "\n",
    "    Args:\n",
    "        fname: NPZ filename.\n",
    "\n",
    "    Returns:\n",
    "        data: Tuple {inputs, target}_{train, valid, test}.\n",
    "              Row-major, outer axis to be the number of observations.\n",
    "    \"\"\"\n",
    "    npzfile = np.load(fname)\n",
    "\n",
    "    inputs_train = npzfile['inputs_train'].T / 255.0\n",
    "    inputs_valid = npzfile['inputs_valid'].T / 255.0\n",
    "    inputs_test = npzfile['inputs_test'].T / 255.0\n",
    "    target_train = npzfile['target_train'].tolist()\n",
    "    target_valid = npzfile['target_valid'].tolist()\n",
    "    target_test = npzfile['target_test'].tolist()\n",
    "\n",
    "    num_class = max(target_train + target_valid + target_test) + 1\n",
    "    target_train_1hot = np.zeros([num_class, len(target_train)])\n",
    "    target_valid_1hot = np.zeros([num_class, len(target_valid)])\n",
    "    target_test_1hot = np.zeros([num_class, len(target_test)])\n",
    "\n",
    "    for ii, xx in enumerate(target_train):\n",
    "        target_train_1hot[xx, ii] = 1.0\n",
    "\n",
    "    for ii, xx in enumerate(target_valid):\n",
    "        target_valid_1hot[xx, ii] = 1.0\n",
    "\n",
    "    for ii, xx in enumerate(target_test):\n",
    "        target_test_1hot[xx, ii] = 1.0\n",
    "\n",
    "    inputs_train = inputs_train.T\n",
    "    inputs_valid = inputs_valid.T\n",
    "    inputs_test = inputs_test.T\n",
    "    target_train_1hot = target_train_1hot.T\n",
    "    target_valid_1hot = target_valid_1hot.T\n",
    "    target_test_1hot = target_test_1hot.T\n",
    "    return inputs_train, inputs_valid, inputs_test, target_train_1hot, target_valid_1hot, target_test_1hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LoadData('./toronto_face.npz')\n",
    "inputs = data[:3]\n",
    "targets = data[3:]\n",
    "inputs = {k:v for k, v in zip(['train', 'valid', 'test'], inputs)}\n",
    "targets = {k:v for k, v in zip(['train', 'valid', 'test'], targets)}\n",
    "\n",
    "print('training dataset')\n",
    "print('inputs:', inputs['train'].shape, 'targets:', targets['train'].shape)\n",
    "print('validation dataset')\n",
    "print('inputs:', inputs['valid'].shape, 'targets:', targets['valid'].shape)\n",
    "print('test dataset')\n",
    "print('inputs:', inputs['test'].shape, 'targets:', targets['test'].shape)\n",
    "\n",
    "classes = ['anger', 'disgust', 'fear', 'happy', 'sad', 'suprise', 'neutral']\n",
    "_, labels = np.nonzero(targets['train'])\n",
    "\n",
    "figs, axes = plt.subplots(nrows=1, ncols=7, figsize=(14,7))\n",
    "for idx in range(7):\n",
    "    axis = axes[idx]\n",
    "    rnd_idx = np.random.choice(np.nonzero(labels == idx)[0])\n",
    "    axis.axis('off')\n",
    "    axis.imshow(inputs['train'][rnd_idx].reshape(48, 48), cmap='gray')\n",
    "    axis.set_title('{}: {}'.format(rnd_idx, classes[idx]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Multi-layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. 기본적인 일반화 (basic generalization): 코드에 주어진 hyperparameter 들을 이용하여 신경망을 학습시킨다. 학습 오차(training error)와 일반화를 위한 검증 오차(validation error) 결과가 어떻게 다른지 설명한다. 두 가지 경우(학습과 일반화 검증)에 대해 오차 커브(error curve)를 그래프로 제시하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 최적화 (optimization):  Learning rate, momentum, mini-batch size 세 가지 종류의 parameter 들을 아래와 같이 변화시키면서 다양한 조합들에 대해 신경망이 cross-entropy 관점에서 어떻게 수렴하는지 살펴본다. 가장 우수한 성능을 나타내는 hyperparameter 들의 조합이 어떤 것인지 제시하시오. (모든 경우의 수를 다 따지면 75 가지 신경망 모델을 테스트해야 하나 시간이 너무 많이 결릴 수 있으므로 이 중에서 일부분의 경우들만 테스트해도 된다. 그러나 어떤 근거로 해당 조합들만 테스트했는지 적당한 설명이 있어야 함.)\n",
    "    - Learning rate ( $\\epsilon$ ): 0.001 에서 1.0 사이의 5 가지 경우\n",
    "    - Momentum: 0.0 에서 0.9 사이의 3 가지 경우\n",
    "    - Mini-batch size: 1 에서 1000 까지의 5 가지 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 신경망 모델 구조 변경: Momentum 을 0.9로 고정시킨 상태에서 신경망의 hidden unit 들의 갯수를 2 에서 100 사이의 3 가지 다른 경우에 대해 성능을 비교한다. 필요한 경우 learning rate 와 학습 기간(epochs)은 신경망 구조에 따라 적당하게 변경할 수 있다. Hidden unit 의 갯수들이 학습에서의 수렴과 신경망의 일반화 성는에 미치는 영향에 대한 데이터(표나 그래프)를 제시하고 경향을 분석하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save(fname, data):\n",
    "    \"\"\"Saves the model to a numpy file.\"\"\"\n",
    "    print('Writing to ' + fname)\n",
    "    np.savez_compressed(fname, **data)\n",
    "\n",
    "\n",
    "def Load(fname):\n",
    "    \"\"\"Loads model from numpy file.\"\"\"\n",
    "    print('Loading from ' + fname)\n",
    "    return dict(np.load(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "아래는 neural networks 의 초기화 및 forward pass를 구현한 코드 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Affine(x, w, b):\n",
    "    \"\"\"Computes the affine transformation.\n",
    "\n",
    "    Args:\n",
    "        x: Inputs\n",
    "        w: Weights\n",
    "        b: Bias\n",
    "\n",
    "    Returns:\n",
    "        y: Outputs\n",
    "    \"\"\"\n",
    "    # y = np.dot(w.T, x) + b\n",
    "    y = x.dot(w) + b\n",
    "    return y\n",
    "\n",
    "def ReLU(x):\n",
    "    \"\"\"Computes the ReLU activation function.\n",
    "\n",
    "    Args:\n",
    "        x: Inputs\n",
    "\n",
    "    Returns:\n",
    "        y: Activation\n",
    "    \"\"\"\n",
    "    return np.maximum(x, 0.0)\n",
    "\n",
    "def Softmax(x):\n",
    "    \"\"\"Computes the softmax activation function.\n",
    "\n",
    "    Args:\n",
    "        x: Inputs\n",
    "\n",
    "    Returns:\n",
    "        y: Activation\n",
    "    \"\"\"\n",
    "    return np.exp(x) / np.exp(x).sum(axis=1, keepdims=True)\n",
    "\n",
    "def InitNN(num_inputs, num_hiddens, num_outputs):\n",
    "    \"\"\"Initializes NN parameters.\n",
    "\n",
    "    Args:\n",
    "        num_inputs:    Number of input units.\n",
    "        num_hiddens:   List of two elements, hidden size for each layer.\n",
    "        num_outputs:   Number of output units.\n",
    "\n",
    "    Returns:\n",
    "        model:         Randomly initialized network weights.\n",
    "    \"\"\"\n",
    "    W1 = 0.1 * np.random.randn(num_inputs, num_hiddens[0])\n",
    "    W2 = 0.1 * np.random.randn(num_hiddens[0], num_hiddens[1])\n",
    "    W3 = 0.01 * np.random.randn(num_hiddens[1], num_outputs)\n",
    "    b1 = np.zeros((num_hiddens[0]))\n",
    "    b2 = np.zeros((num_hiddens[1]))\n",
    "    b3 = np.zeros((num_outputs))\n",
    "    model = {\n",
    "        'W1': W1,\n",
    "        'W2': W2,\n",
    "        'W3': W3,\n",
    "        'b1': b1,\n",
    "        'b2': b2,\n",
    "        'b3': b3\n",
    "    }\n",
    "    return model\n",
    "\n",
    "def NNForward(model, x):\n",
    "    \"\"\"Runs the forward pass.\n",
    "\n",
    "    Args:\n",
    "        model: Dictionary of all the weights.\n",
    "        x:     Input to the network.\n",
    "\n",
    "    Returns:\n",
    "        var:   Dictionary of all intermediate variables.\n",
    "    \"\"\"\n",
    "    h1 = Affine(x, model['W1'], model['b1'])\n",
    "    h1r = ReLU(h1)\n",
    "    h2 = Affine(h1r, model['W2'], model['b2'])\n",
    "    h2r = ReLU(h2)\n",
    "    y = Affine(h2r, model['W3'], model['b3'])\n",
    "    var = {\n",
    "        'x': x,\n",
    "        'h1': h1,\n",
    "        'h1r': h1r,\n",
    "        'h2': h2,\n",
    "        'h2r': h2r,\n",
    "        'y': y\n",
    "    }\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 neural networks 의 backward 구현하기 위한 코드들입니다.\n",
    "아래 세 부분을 채워 코드를 완성시키기 바랍니다.\n",
    "\n",
    "1. Affine layer 의 backward pass equations (linear trainsformation + bias).\n",
    "2. RELU activation function 의 backward pass equations.\n",
    "3. Momentum 이 포함된 weight update equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AffineBackward(grad_y, x, w):\n",
    "    \"\"\"Computes gradients of affine transformation.\n",
    "\n",
    "    Args:\n",
    "        grad_y: gradient from last layer\n",
    "        x: inputs\n",
    "        w: weights\n",
    "\n",
    "    Returns:\n",
    "        grad_x: Gradients wrt. the inputs.\n",
    "        grad_w: Gradients wrt. the weights.\n",
    "        grad_b: Gradients wrt. the biases.\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    # Insert your code here.\n",
    "    # grad_x = ...\n",
    "    # grad_w = ...\n",
    "    # grad_b = ...\n",
    "    # return grad_x, grad_w, grad_b\n",
    "    ###########################\n",
    "    raise Exception('Not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLUBackward(grad_y, x, y):\n",
    "    \"\"\"Computes gradients of the ReLU activation function.\n",
    "\n",
    "    Returns:\n",
    "        grad_x: Gradients wrt. the inputs.\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    # Insert your code here.\n",
    "    # grad_x = ...\n",
    "    # return grad_x\n",
    "    ###########################\n",
    "    raise Exception('Not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNBackward(model, err, var):\n",
    "    \"\"\"Runs the backward pass.\n",
    "\n",
    "    Args:\n",
    "        model:    Dictionary of all the weights.\n",
    "        err:      Gradients to the output of the network.\n",
    "        var:      Intermediate variables from the forward pass.\n",
    "    \"\"\"\n",
    "    dE_dh2r, dE_dW3, dE_db3 = AffineBackward(err, var['h2r'], model['W3'])\n",
    "    dE_dh2 = ReLUBackward(dE_dh2r, var['h2'], var['h2r'])\n",
    "    dE_dh1r, dE_dW2, dE_db2 = AffineBackward(dE_dh2, var['h1r'], model['W2'])\n",
    "    dE_dh1 = ReLUBackward(dE_dh1r, var['h1'], var['h1r'])\n",
    "    _, dE_dW1, dE_db1 = AffineBackward(dE_dh1, var['x'], model['W1'])\n",
    "    model['dE_dW1'] = dE_dW1\n",
    "    model['dE_dW2'] = dE_dW2\n",
    "    model['dE_dW3'] = dE_dW3\n",
    "    model['dE_db1'] = dE_db1\n",
    "    model['dE_db2'] = dE_db2\n",
    "    model['dE_db3'] = dE_db3\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNUpdate(model, eps, momentum):\n",
    "    \"\"\"Update NN weights.\n",
    "\n",
    "    Args:\n",
    "        model:    Dictionary of all the weights.\n",
    "        eps:      Learning rate.\n",
    "        momentum: Momentum.\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    # Insert your code here.\n",
    "    # Update the weights.\n",
    "    # model['W1'] = ...\n",
    "    # model['W2'] = ...\n",
    "    # model['W3'] = ...\n",
    "    # model['b1'] = ...\n",
    "    # model['b2'] = ...\n",
    "    # model['b3'] = ...\n",
    "    ###########################\n",
    "    raise Exception('Not implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model, forward, backward, update, eps, momentum, num_epochs,\n",
    "          batch_size):\n",
    "    \"\"\"Trains a simple MLP.\n",
    "\n",
    "    Args:\n",
    "        model:           Dictionary of model weights.\n",
    "        forward:         Forward prop function.\n",
    "        backward:        Backward prop function.\n",
    "        update:          Update weights function.\n",
    "        eps:             Learning rate.\n",
    "        momentum:        Momentum.\n",
    "        num_epochs:      Number of epochs to run training for.\n",
    "        batch_size:      Mini-batch size, -1 for full batch.\n",
    "\n",
    "    Returns:\n",
    "        stats:           Dictionary of training statistics.\n",
    "            - train_ce:       Training cross entropy.\n",
    "            - valid_ce:       Validation cross entropy.\n",
    "            - train_acc:      Training accuracy.\n",
    "            - valid_acc:      Validation accuracy.\n",
    "    \"\"\"\n",
    "    inputs_train, inputs_valid, inputs_test, target_train, target_valid, \\\n",
    "        target_test = LoadData('./toronto_face.npz')\n",
    "    rnd_idx = np.arange(inputs_train.shape[0])\n",
    "    train_ce_list = []\n",
    "    valid_ce_list = []\n",
    "    train_acc_list = []\n",
    "    valid_acc_list = []\n",
    "    \n",
    "    num_train_cases = inputs_train.shape[0]\n",
    "    if batch_size == -1:\n",
    "        batch_size = num_train_cases\n",
    "    num_steps = int(np.ceil(num_train_cases / batch_size))\n",
    "\n",
    "    pp = ProgressPlot(\n",
    "        plot_names=['Cross entropy', 'Accuracy'],\n",
    "        line_names=['Train', 'Validation'],\n",
    "        x_label='Iteration',\n",
    "        x_lim=[0, num_epochs*num_steps]\n",
    "    )\n",
    "\n",
    "    valid_ce = 0\n",
    "    valid_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        np.random.shuffle(rnd_idx)\n",
    "        inputs_train = inputs_train[rnd_idx]\n",
    "        target_train = target_train[rnd_idx]\n",
    "        for step in range(num_steps):\n",
    "            # Forward prop.\n",
    "            start = step * batch_size\n",
    "            end = min(num_train_cases, (step + 1) * batch_size)\n",
    "            x = inputs_train[start: end]\n",
    "            t = target_train[start: end]\n",
    "\n",
    "            var = forward(model, x)\n",
    "            prediction = Softmax(var['y'])\n",
    "\n",
    "            train_ce = -np.sum(t * np.log(prediction)) / x.shape[0]\n",
    "            train_acc = (np.argmax(prediction, axis=1) ==\n",
    "                         np.argmax(t, axis=1)).astype('float').mean()\n",
    "            pp.update([[train_ce, valid_ce], [train_acc, valid_acc]])\n",
    "\n",
    "            # Compute error.\n",
    "            error = (prediction - t) / x.shape[0]\n",
    "\n",
    "            # Backward prop.\n",
    "            backward(model, error, var)\n",
    "\n",
    "            # Update weights.\n",
    "            update(model, eps, momentum)\n",
    "\n",
    "        valid_ce, valid_acc = Evaluate(\n",
    "            inputs_valid, target_valid, model, forward, batch_size=batch_size)\n",
    "        \n",
    "        pp.update([[train_ce, valid_ce], [train_acc, valid_acc]])\n",
    "        train_ce_list.append((epoch, train_ce))\n",
    "        train_acc_list.append((epoch, train_acc))\n",
    "        valid_ce_list.append((epoch, valid_ce))\n",
    "        valid_acc_list.append((epoch, valid_acc))\n",
    "\n",
    "    # print()\n",
    "    train_ce, train_acc = Evaluate(\n",
    "        inputs_train, target_train, model, forward, batch_size=batch_size)\n",
    "    valid_ce, valid_acc = Evaluate(\n",
    "        inputs_valid, target_valid, model, forward, batch_size=batch_size)\n",
    "    test_ce, test_acc = Evaluate(\n",
    "        inputs_test, target_test, model, forward, batch_size=batch_size)\n",
    "    print('CE: Train %.5f Validation %.5f Test %.5f' %\n",
    "          (train_ce, valid_ce, test_ce))\n",
    "    print('Acc: Train {:.5f} Validation {:.5f} Test {:.5f}'.format(\n",
    "        train_acc, valid_acc, test_acc))\n",
    "    pp.finalize()\n",
    "    stats = {\n",
    "        'train_ce': train_ce_list,\n",
    "        'valid_ce': valid_ce_list,\n",
    "        'train_acc': train_acc_list,\n",
    "        'valid_acc': valid_acc_list\n",
    "    }\n",
    "\n",
    "    return model, stats\n",
    "\n",
    "def Evaluate(inputs, target, model, forward, batch_size=-1):\n",
    "    \"\"\"Evaluates the model on inputs and target.\n",
    "\n",
    "    Args:\n",
    "        inputs: Inputs to the network.\n",
    "        target: Target of the inputs.\n",
    "        model:  Dictionary of network weights.\n",
    "    \"\"\"\n",
    "    num_cases = inputs.shape[0]\n",
    "    if batch_size == -1:\n",
    "        batch_size = num_cases\n",
    "    num_steps = int(np.ceil(num_cases / batch_size))\n",
    "    ce = 0.0\n",
    "    acc = 0.0\n",
    "    for step in range(num_steps):\n",
    "        start = step * batch_size\n",
    "        end = min(num_cases, (step + 1) * batch_size)\n",
    "        x = inputs[start: end]\n",
    "        t = target[start: end]\n",
    "        prediction = Softmax(forward(model, x)['y'])\n",
    "        ce += -np.sum(t * np.log(prediction))\n",
    "        acc += (np.argmax(prediction, axis=1) == np.argmax(\n",
    "            t, axis=1)).astype('float').sum()\n",
    "    ce /= num_cases\n",
    "    acc /= num_cases\n",
    "    return ce, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckGrad(model, forward, backward, name, x):\n",
    "    \"\"\"Check the gradients\n",
    "\n",
    "    Args:\n",
    "        model: Dictionary of network weights.\n",
    "        name: Weights name to check.\n",
    "        x: Fake input.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    var = forward(model, x)\n",
    "    loss = lambda y: 0.5 * (y ** 2).sum()\n",
    "    grad_y = var['y']\n",
    "    backward(model, grad_y, var)\n",
    "    grad_w = model['dE_d' + name].ravel()\n",
    "    w_ = model[name].ravel()\n",
    "    eps = 1e-7\n",
    "    grad_w_2 = np.zeros(w_.shape)\n",
    "    check_elem = np.arange(w_.size)\n",
    "    np.random.shuffle(check_elem)\n",
    "    # Randomly check 20 elements.\n",
    "    check_elem = check_elem[:20]\n",
    "    for ii in check_elem:\n",
    "        w_[ii] += eps\n",
    "        err_plus = loss(forward(model, x)['y'])\n",
    "        w_[ii] -= 2 * eps\n",
    "        err_minus = loss(forward(model, x)['y'])\n",
    "        w_[ii] += eps\n",
    "        grad_w_2[ii] = (err_plus - err_minus) / 2 / eps\n",
    "    np.testing.assert_almost_equal(grad_w[check_elem], grad_w_2[check_elem],\n",
    "                                   decimal=3)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Trains a NN.\"\"\"\n",
    "    model_fname = 'nn_model.npz'\n",
    "    stats_fname = 'nn_stats.npz'\n",
    "\n",
    "    # Hyper-parameters. Modify them if needed.\n",
    "    num_hiddens = [16, 32]\n",
    "    eps = 0.01\n",
    "    momentum = 0.0\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "\n",
    "    # Input-output dimensions.\n",
    "    num_inputs = 2304\n",
    "    num_outputs = 7\n",
    "\n",
    "    # Initialize model.\n",
    "    model = InitNN(num_inputs, num_hiddens, num_outputs)\n",
    "\n",
    "    # Uncomment to reload trained model here.\n",
    "    # model = Load(model_fname)\n",
    "\n",
    "    # Check gradient implementation.\n",
    "    print('Checking gradients...')\n",
    "    x = np.random.rand(10, 48 * 48) * 0.1\n",
    "    CheckGrad(model, NNForward, NNBackward, 'W3', x)\n",
    "    CheckGrad(model, NNForward, NNBackward, 'b3', x)\n",
    "    CheckGrad(model, NNForward, NNBackward, 'W2', x)\n",
    "    CheckGrad(model, NNForward, NNBackward, 'b2', x)\n",
    "    CheckGrad(model, NNForward, NNBackward, 'W1', x)\n",
    "    CheckGrad(model, NNForward, NNBackward, 'b1', x)\n",
    "\n",
    "    # Train model.\n",
    "    trained_model, stats = Train(model, NNForward, NNBackward, NNUpdate, eps,\n",
    "                  momentum, num_epochs, batch_size)\n",
    "\n",
    "    plt.figure(0)\n",
    "    plt.plot(np.array(stats['train_ce'])[:, 0], np.array(stats['train_ce'])[:, 1], 'b', label='Train')\n",
    "    plt.plot(np.array(stats['valid_ce'])[:, 0], np.array(stats['valid_ce'])[:, 1], 'orange', label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(np.array(stats['train_acc'])[:, 0], np.array(stats['train_acc'])[:, 1], 'b', label='Train')\n",
    "    plt.plot(np.array(stats['valid_acc'])[:, 0], np.array(stats['valid_acc'])[:, 1], 'orange', label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Uncomment if you wish to save the model.\n",
    "    # Save(model_fname, model)\n",
    "\n",
    "    # Uncomment if you wish to save the training statistics.\n",
    "    # Save(stats_fname, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
